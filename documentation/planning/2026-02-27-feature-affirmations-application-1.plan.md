---
name: Affirmations Application — LangChain + Ollama Jupyter Notebook Project
description: Scaffold a new Python application in the monorepo that generates structured affirmations for spiritual practices using LangChain, LangGraph, Ollama (Gemma 3), and Jupyter notebooks.
created: 2026-02-27
updated: 2026-02-28
revision: 7
status: "Completed"
---

# Introduction

![Status: Completed](https://img.shields.io/badge/status-Completed-brightgreen)

Create a new `affirmations` application in `applications/affirmations/` that uses Jupyter notebooks with LangChain and LangGraph to generate affirmations in strict grammatical structures for spiritual practices including tarot, lenormand cards, astrology, chakras, kabbalah, runes, and more. The application connects to a local Gemma 3 LLM running on Ollama in a Docker container (via Docker-in-Docker). Output is structured JSON files organized by practice. Python environment is managed with `uv`.

This is the **first Python project** in the monorepo, establishing patterns for Python packaging, linting (Ruff), type-checking (pyright), dead code analysis (vulture), testing (pytest), and Nx integration that future Python projects will follow.

## 1. Requirements & Constraints

### Functional Requirements

- **REQ-001**: Generate affirmations for multiple spiritual practices: tarot, lenormand, astrology, chakras, kabbalah, runes, and extensible to more
- **REQ-002**: Affirmations must follow strict grammatical structures (e.g., "I am (positive quality) through (transformative process)")
- **REQ-003**: Output affirmations as structured JSON files organized by practice (`output/tarot.json`, `output/astrology.json`, etc.)
- **REQ-004**: Provide a working Jupyter notebook with an example prompt to the LLM demonstrating the affirmation generation flow
- **REQ-005**: Connect to Ollama running Gemma 3 (4B parameter model) in a Docker container
- **REQ-006**: Use LangChain (LCEL pipe syntax) for prompt → model → parser chains
- **REQ-007**: Use Pydantic models for structured output validation of generated affirmations
- **REQ-008**: Provide the LLM with web search tools for researching spiritual practices, grammatical structures, symbolism, and historical context in real time. Primary: **SearxNG** (self-hosted, no API key, aggregates 135+ engines including Wikipedia, DuckDuckGo, Google Scholar, and ArXiv). DuckDuckGo has been removed as a standalone tool in favor of SearxNG, which subsumes it via its metasearch engine aggregation
- **REQ-009**: Provide the LLM with a Wikipedia lookup tool for authoritative reference material on tarot, astrology, kabbalah, runes, chakras, lenormand, grammar, and related topics
- **REQ-010**: Use a LangGraph ReAct agent to orchestrate tool usage — the agent decides when to search the web or consult Wikipedia before generating affirmations
- **REQ-011**: Provide a **SearxNG** metasearch engine as a self-hosted Docker Compose service. SearxNG aggregates 135+ search engines (including Wikipedia, Google Scholar, ArXiv, GitHub) via a single unified API with no API keys, no rate limits, and engine-specific targeting (e.g., `engines=["wiki"]`, `categories="science"`)
- **REQ-012**: Implement a **research processing layer** (`research.py`) that transforms raw search results and web content into LLM-digestible context for the 4B model. Uses **Trafilatura** (best-in-class open-source HTML→text extractor, used by HuggingFace/IBM/Stanford) for content extraction instead of basic BeautifulSoup tag stripping. Truncates to relevant sections, deduplicates across sources, formats results into a consistent structure, and enforces a context budget (e.g., max 3,000 tokens of research context) to prevent overwhelming the 4B model's limited attention
- **REQ-014**: Provide an **Open WebUI** browser interface for interacting with the Ollama model directly without writing code. Open WebUI runs as a Docker Compose service and connects to the `ollama` service within the Docker network. An Nx target `open-webui` opens `http://localhost:3000` in VSCode's integrated browser via `$BROWSER`
- **REQ-015**: Implement **dead code analysis** using `vulture` to detect unused functions, variables, and classes in the Python source. Add a `vulture` Nx target running `uv run vulture src/`. A whitelist file handles Pydantic model fields and LangChain duck-typing patterns that vulture may false-positive on

### Security Requirements

- **SEC-001**: Ollama must bind only to `0.0.0.0` within the Docker network, not exposed to the public internet
- **SEC-002**: Disable Ollama cloud features (`OLLAMA_NO_CLOUD=1`) for fully local operation
- **SEC-003**: No API keys or secrets required for core functionality — all inference is local. SearxNG (self-hosted, no key) and Wikipedia (public API, no key) provide full search capability without commercial API dependencies. DuckDuckGo and Jina Reader have been removed to eliminate all external API dependencies

### Constraints

- **CON-001**: Must integrate with the existing Nx monorepo build system using `project.json` and `nx:run-commands` executor
- **CON-002**: Python package management via `uv` (no pip/poetry) — use `pyproject.toml` and `uv.lock`
- **CON-003**: The devcontainer already has Python 3.14, JupyterLab, Docker-in-Docker, and Ruff — leverage these, do not duplicate
- **CON-004**: Ollama runs as a Docker Compose service (Docker-in-Docker within the devcontainer)
- **CON-005**: First Python project in the monorepo — all TypeScript-centric target defaults (`biome`, `eslint`, `oxlint`, `typecheck`, `type-coverage`) must be excluded or overridden with Python equivalents
- **CON-006**: The `applications/affirmations/` directory already exists but is completely empty
- **CON-007**: Gemma 3 4B model is 3.3GB — model download is a manual step, not baked into Docker images
- **CON-008**: CPU-only inference in Docker-in-Docker (no GPU passthrough) — expect slower generation times
- **CON-009**: Python ≥ 3.11 required for LangGraph CLI compatibility (devcontainer has 3.14)

### Guidelines

- **GUD-001**: Follow the existing `caelundas` project as the structural analog for Nx project configuration
- **GUD-002**: Follow the monorepo's `documentation/frameworks/langchain-python.md` guide for LangChain patterns (LCEL, Runnables, `ChatOllama`)
- **GUD-003**: Use `ChatOllama` (not `OllamaLLM`) for all chat/instruction workloads per LangChain best practices
- **GUD-004**: Use LCEL pipe syntax (`prompt | model | parser`) for simple chains — not deprecated `LLMChain`. For tool-augmented workflows, use LangGraph `create_react_agent` or `StateGraph`
- **GUD-005**: Structured outputs via `model.with_structured_output(PydanticModel)` for typed affirmation generation
- **GUD-008**: Web search and Wikipedia tools are LangChain `Tool` instances from `langchain-community` (SearxNG, Wikipedia). They are bound to the LLM via `llm.bind_tools()` and orchestrated by a LangGraph ReAct agent loop. DuckDuckGo and Jina Reader have been removed — SearxNG subsumes DuckDuckGo via metasearch, and Trafilatura covers all content extraction locally
- **GUD-009**: The agent should research first (search/Wikipedia/SearxNG), then generate affirmations informed by the research — this is a two-phase workflow within the graph
- **GUD-010**: All tool outputs pass through the research processing layer (`research.py`) before reaching the LLM. Raw search results are extracted by **Trafilatura** (not basic tag stripping), then truncated, deduplicated, and formatted into a consistent structure. This is critical for the 4B model which has limited context processing ability
- **GUD-006**: Create `AGENTS.md` for the project following existing conventions
- **GUD-007**: All domain-specific terms must be added to `cspell.config.yaml`

### Patterns

- **PAT-001**: Nx project targets use `executor: "nx:run-commands"` with `command` and `cwd` for Python tooling
- **PAT-002**: Language-agnostic targets (`markdown-lint`, `spell-check`, `yaml-lint`) inherit from `nx.json` `targetDefaults` with empty `{}`
- **PAT-003**: Python-specific targets replace TypeScript equivalents: `ruff` → lint/format, `pyright` → typecheck, `vulture` → vulture, `pytest` → test
- **PAT-004**: Docker Compose service pattern follows existing `caelundas` service structure in `docker-compose.yml`
- **PAT-005**: JSON output files stored in `output/` directory (matching caelundas pattern)

## 2. Implementation Steps

### Implementation Phase 1 — Project Scaffolding & Monorepo Integration

- GOAL-001: Create the foundational project structure, integrate with Nx, and update all monorepo-wide configuration files.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Completed | Date       |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------- | ---------- |
| TASK-001 | Create `applications/affirmations/project.json` with Nx project configuration. Set `name: "affirmations"`, `sourceRoot: "applications/affirmations/src"`, `projectType: "application"`, `tags: ["scope:affirmations", "type:application", "lang:python"]`. Define targets in TASK-003.                                                                                                                                                                                                                                                                                   | ✅        | 2026-02-28 |
| TASK-002 | Create `applications/affirmations/AGENTS.md` describing the project: Python application, LangChain + LangGraph, Ollama Gemma 3, Jupyter notebook-based affirmation generator. Include project-specific conventions, commands, and file structure.                                                                                                                                                                                                                                                                                                                        | ✅        | 2026-02-28 |
| TASK-003 | Define Nx targets in `project.json`: `lint` → `uv run ruff check .`, `format` → `uv run ruff format .`, `typecheck` → `uv run pyright src/` (updated from `mypy` — see TASK-040), `vulture` → `uv run vulture src/ --min-confidence 80` (new — see TASK-041), `open-webui` → `$BROWSER http://localhost:3000` (new — see TASK-045), `open-searxng` → `$BROWSER http://localhost:8889` (new — see TASK-045), `test` → `uv run pytest`, `notebook` → `uv run jupyter lab`, `spell-check` → `{}` (inherit), `markdown-lint` → `{}` (inherit), `yaml-lint` → `{}` (inherit). | ✅        | 2026-02-28 |
| TASK-004 | Add `"affirmations"` to the `scopes` array in `conventional.config.cjs` for commit message validation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | ✅        | 2026-02-28 |
| TASK-005 | Add domain terms to `cspell.config.yaml` dictionary: `ollama`, `langchain`, `langgraph`, `gemma`, `tarot`, `lenormand`, `kabbalah`, `chakra`, `chakras`, `rune`, `runes`, `astrology`, `affirmations`, `pydantic`, `ipykernel`, `pyproject`, `pyright`, `vulture`, `pytest`, `ruff`, `wikipedia`, `reagent`, `reactagent`, `searxng`, `metasearch`, `trafilatura`, `openwebui`.                                                                                                                                                                                          | ✅        | 2026-02-28 |
| TASK-006 | Add `applications/affirmations` workspace entry to `knip.config.ts`. Since this is a Python project, add it to `ignoreWorkspaces` array (Knip does not analyze Python).                                                                                                                                                                                                                                                                                                                                                                                                  | ✅        | 2026-02-28 |
| TASK-007 | Create `applications/affirmations/.gitignore` with Python-specific ignores: `__pycache__/`, `*.pyc`, `.ruff_cache/`, `.vulture_whitelist.py` is tracked, `.pytest_cache/`, `.venv/`, `*.egg-info/`, `dist/`, `.ipynb_checkpoints/`, `output/*.json` (generated output).                                                                                                                                                                                                                                                                                                  | ✅        | 2026-02-28 |
| TASK-008 | Create directory structure: `applications/affirmations/src/` (Python source), `applications/affirmations/notebooks/` (Jupyter notebooks), `applications/affirmations/output/` (generated JSON), `applications/affirmations/testing/` (test files).                                                                                                                                                                                                                                                                                                                       | ✅        | 2026-02-28 |

### Implementation Phase 2 — Python Environment & Dependencies

- GOAL-002: Set up Python packaging with `uv`, install all dependencies, and configure Python tooling (Ruff, pyright, vulture, pytest).

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Completed | Date       |
| -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---------- |
| TASK-009 | Create `applications/affirmations/pyproject.toml` with: `[project]` metadata (name=`affirmations`, version=`0.1.0`, requires-python=`>=3.11`), dependencies: `langchain~=1.2`, `langchain-ollama~=1.0`, `langchain-community~=1.0`, `langgraph~=1.0`, `pydantic~=2.0`, `jupyter~=1.1`, `ipykernel~=6.29`, `wikipedia~=1.4`, `trafilatura~=2.0`. Dev dependencies: `ruff`, `pyright`, `vulture`, `pytest`, `pytest-cov`. **Deviation from original plan**: Removed `duckduckgo-search~=7.0` (DuckDuckGo removed in favor of SearxNG), removed `langchain-community[jina]` optional dependency (Jina removed in favor of Trafilatura-only), replaced `mypy` with `pyright`. Used `>=` version specifiers instead of `~=` for broader compatibility with latest releases. | ✅        | 2026-02-28 |
| TASK-010 | Configure Ruff in `pyproject.toml` `[tool.ruff]` section: `target-version = "py311"`, `line-length = 100`, enable rules `E`, `F`, `I` (isort), `UP` (pyupgrade), `B` (bugbear), `SIM` (simplify), `TCH` (type-checking imports). Set `[tool.ruff.format]` quote-style to `"double"`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | ✅        | 2026-02-28 |
| TASK-011 | Configure **pyright** (replaces mypy) in `pyproject.toml` `[tool.pyright]` section: `pythonVersion = "3.11"`, `typeCheckingMode = "strict"`, `venvPath = "."`, `venv = ".venv"`. Remove `[tool.mypy]` and `[[tool.mypy.overrides]]` sections. Update dev dependencies to include `pyright` and remove `mypy`. **Deviation from original plan**: pyright provides equivalent type checking with better IDE integration, faster performance, and no need for per-package `[[overrides]]` stubs for LangChain/LangGraph.                                                                                                                                                                                                                                                  | ✅        | 2026-02-28 |
| TASK-012 | Configure pytest in `pyproject.toml` `[tool.pytest.ini_options]` section: `testpaths = ["testing"]`, `python_files = ["test_*.py"]`, `python_classes = ["Test*"]`, `python_functions = ["test_*"]`, `addopts = "--cov=src --cov-report=term-missing --cov-fail-under=80"`, markers: `unit`, `integration`.                                                                                                                                                                                                                                                                                                                                                                                                                                                             | ✅        | 2026-02-28 |
| TASK-013 | Run `uv sync` in `applications/affirmations/` to create `uv.lock` and `.venv/`. Register the Jupyter kernel: `uv run python -m ipykernel install --user --name affirmations --display-name "Affirmations (Python)"`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | ✅        | 2026-02-28 |

### Implementation Phase 3 — Ollama Docker Service

- GOAL-003: Add Ollama as a Docker Compose service and set up the model pull workflow.

| Task      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Completed | Date       |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------- | ---------- |
| TASK-014  | Add `ollama` service to `docker-compose.yml`: image `ollama/ollama:latest`, container name `ollama`, ports `11434:11434`, volume `ollama_data:/root/.ollama`, environment variables `OLLAMA_HOST=0.0.0.0:11434`, `OLLAMA_KEEP_ALIVE=10m`, `OLLAMA_NUM_PARALLEL=1`, `OLLAMA_MAX_LOADED_MODELS=1`, `OLLAMA_NO_CLOUD=1`, restart policy `unless-stopped`. Add `ollama_data` named volume.                                                                                                                                                                                                                                                     | ✅        | 2026-02-28 |
| TASK-014b | Add `searxng` service to `docker-compose.yml`: image `searxng/searxng:latest`, container name `searxng`, ports `8888:8080`, volumes: `./applications/affirmations/config/searxng:/etc/searxng` (bind mount for settings), environment `SEARXNG_BASE_URL=http://localhost:8888`, restart `unless-stopped`. The SearxNG service provides a self-hosted metasearch API aggregating 135+ engines with no API keys. **Deviation**: Port changed to `8889:8080` (host port 8889) to avoid conflict with JupyterLab's default port 8888. `SEARXNG_BASE_URL` set to `http://localhost:8889` accordingly.                                           | ✅        | 2026-02-28 |
| TASK-014c | Create `applications/affirmations/config/searxng/settings.yml`: SearxNG configuration enabling relevant engines (`wikipedia`, `duckduckgo`, `google scholar`, `arxiv`, `github`, `wikidata`), setting JSON as default output format, disabling engines that require API keys, setting `server.secret_key`, and configuring rate limiting for responsible usage.                                                                                                                                                                                                                                                                            | ✅        | 2026-02-28 |
| TASK-015  | Add Nx target `ollama:start` in `applications/affirmations/project.json`: command `docker compose -f ../../docker-compose.yml up -d ollama`, cwd `applications/affirmations`. Add `ollama:stop` target: command `docker compose -f ../../docker-compose.yml stop ollama`. Add `ollama:pull` target: command `docker exec ollama ollama pull gemma3:4b`.                                                                                                                                                                                                                                                                                    | ✅        | 2026-02-28 |
| TASK-016  | Add port `11434` to `forwardPorts` in `.devcontainer/devcontainer.json` with label `"Ollama API"` if not already present. Add `"OLLAMA_HOST": "http://localhost:11434"` to `remoteEnv` for consistent access from within the devcontainer.                                                                                                                                                                                                                                                                                                                                                                                                 | ✅        | 2026-02-28 |
| TASK-047  | Auto-start `ollama`, `searxng`, and `open-webui` Docker Compose services when the devcontainer starts. Add `docker compose up -d ollama searxng open-webui` to `.devcontainer/scripts/post-create-command.sh` so all three services are running immediately after the container is created. Services keep `restart: unless-stopped` in `docker-compose.yml` so they auto-recover if the inner Docker daemon restarts. Also add port `8889` to `forwardPorts` and `portsAttributes` in `.devcontainer/devcontainer.json` with label `"SearxNG"` and `onAutoForward: "notify"` (port was missing despite being used by the SearxNG service). | ✅        | 2026-02-28 |
| TASK-017  | Create `applications/affirmations/scripts/setup-ollama.sh`: a convenience script that runs `docker compose up -d ollama searxng`, waits for both Ollama (`curl --retry 10 --retry-delay 2 http://localhost:11434/api/tags`) and SearxNG to be healthy, then pulls `gemma3:4b`. Add Nx target `setup` → `bash scripts/setup-ollama.sh`. **Deviation**: SearxNG health check uses port 8889. Script uses a loop-until-ready pattern (not `--retry`).                                                                                                                                                                                         | ✅        | 2026-02-28 |

### Implementation Phase 4 — Source Code & Example Notebook

- GOAL-004: Create the Python source modules and a working Jupyter notebook demonstrating affirmation generation via LangChain → Ollama.

| Task      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Completed | Date       |
| --------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---------- |
| TASK-018  | Create `applications/affirmations/src/__init__.py` (empty, marks package).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | ✅        | 2026-02-28 |
| TASK-019  | Create `applications/affirmations/src/models.py`: Define Pydantic models — `Affirmation(text: str, practice: str, structure: str, keywords: list[str])`, `AffirmationSet(practice: str, affirmations: list[Affirmation])`, `ResearchResult(query: str, source: str, summary: str)`. These enforce the typed output schema for structured LLM generation and research tool outputs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | ✅        | 2026-02-28 |
| TASK-020  | Create `applications/affirmations/src/llm.py`: Factory function `create_llm(model: str = "gemma3:4b", base_url: str = "http://localhost:11434") -> ChatOllama` that returns a configured `ChatOllama` instance. Use `temperature=0.7` default.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | ✅        | 2026-02-28 |
| TASK-021  | Create `applications/affirmations/src/tools.py`: Define LangChain tools for the agent. (a) `searxng_search` — wraps `SearxSearchWrapper` from `langchain_community.utilities` targeting the self-hosted SearxNG instance at `http://localhost:8889`, enabling engine-specific queries (e.g., `engines=["wiki"]`, `engines=["arxiv"]`, `categories="science"`). (b) `wikipedia_lookup` — wraps `WikipediaQueryRun` with `WikipediaAPIWrapper` from `langchain_community.utilities` for authoritative reference material. Factory function `create_tools() -> list[BaseTool]` returns `[searxng_search, wikipedia_lookup]`. **DuckDuckGo removed**: SearxNG subsumes it via metasearch. **Jina Reader removed**: Trafilatura in `research.py` handles all URL content extraction locally without external API dependencies. Each tool wraps its raw output through the research processing layer (TASK-021b).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | ✅        | 2026-02-28 |
| TASK-021b | Create `applications/affirmations/src/research.py`: Research processing layer that transforms raw search/web content into LLM-digestible context for the 4B model. Uses **Trafilatura** for content extraction (best-in-class open-source HTML→text/markdown extractor, outperforms BeautifulSoup in all benchmarks). Implements: (a) `extract_content(raw: str) -> str` — uses `trafilatura.extract()` to pull main content from raw HTML, removing boilerplate (navs, footers, ads, sidebars) and outputting clean text with metadata. Falls back to basic tag stripping if Trafilatura returns empty. (b) `truncate_with_relevance(text: str, query: str, max_chars: int = 1000) -> str` — keeps the most query-relevant section of long text using keyword overlap scoring. (c) `format_research_result(title: str, source: str, content: str) -> str` — formats a single result into a consistent template: `[Source: {source}] {title}: {content}`. (d) `deduplicate_results(results: list[dict]) -> list[dict]` — merges overlapping information across sources using content similarity. (e) `budget_context(results: list[str], max_total_tokens: int = 3000) -> str` — caps total research context to preserve context window for generation, distributing budget across sources proportionally. (f) `process_search_results(raw_results: str, query: str, source_name: str) -> str` — main entry point composing extract → truncate → format → budget for a single tool's output. All functions are pure/stateless for easy testing. | ✅        | 2026-02-28 |
| TASK-022  | Create `applications/affirmations/src/chains.py`: Define `create_affirmation_chain(llm: ChatOllama) -> RunnableSequence` as a simple LCEL chain (`ChatPromptTemplate` → `llm.with_structured_output(Affirmation)`) for direct generation without research. The prompt template accepts `{practice}`, `{topic}`, and `{structure}` variables.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | ✅        | 2026-02-28 |
| TASK-023  | Create `applications/affirmations/src/agent.py`: Define `create_research_agent(llm: ChatOllama, tools: list[BaseTool]) -> CompiledGraph` using LangGraph. Build a `StateGraph` with nodes: (1) `research` — agent uses SearxNG and Wikipedia tools to gather context about the practice/topic (all tool outputs pass through the `research.py` processing layer with Trafilatura extraction), (2) `generate` — agent produces the affirmation informed by processed research results. Use `create_react_agent` from `langgraph.prebuilt` with the tools bound to the LLM. System prompt instructs the agent to first research the topic using available tools, then generate an affirmation in the specified grammatical structure. The system prompt also notes that research results are pre-processed and condensed — the agent should trust the summaries rather than requesting redundant searches.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | ✅        | 2026-02-28 |
| TASK-024  | Create `applications/affirmations/src/output.py`: Utility functions `save_affirmations(affirmations: AffirmationSet, output_dir: Path) -> Path` (writes JSON to `output/{practice}.json`) and `load_affirmations(practice: str, output_dir: Path) -> AffirmationSet` (reads JSON from `output/{practice}.json`). Use `pydantic`'s `.model_dump_json(indent=2)` for serialization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | ✅        | 2026-02-28 |
| TASK-025  | Create `applications/affirmations/src/practices.py`: Define practice configuration as a dict mapping practice names to their metadata — `PRACTICES: dict[str, PracticeConfig]` where `PracticeConfig` is a Pydantic model with fields: `name: str`, `topics: list[str]`, `structures: list[str]`. Seed with initial data for tarot (Major Arcana topics), astrology (zodiac signs), and chakras (7 chakras).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | ✅        | 2026-02-28 |
| TASK-026  | Create example notebook `applications/affirmations/notebooks/example-affirmation-generation.ipynb` with cells: (1) Markdown intro explaining the project, the multi-tool research pipeline, and the Trafilatura-powered processing layer, (2) imports and LLM setup using `create_llm()`, (3) create tools using `create_tools()` — show all available tools (SearxNG, Wikipedia), (4) demonstrate the research processing layer — show raw HTML vs. Trafilatura-extracted content vs. final processed output side-by-side, (5) create research agent using `create_research_agent(llm, tools)`, (6) invoke agent for a single tarot affirmation — demonstrate the agent researching "The Tower card" via SearxNG/Wikipedia then generating an affirmation, (7) show the agent's tool call trace (research steps visible in message history, processed results visible), (8) demonstrate SearxNG engine targeting: `engines=["wiki"]` for Wikipedia, `engines=["arxiv"]` for academic papers, `categories="science"` for scientific sources, (9) also demonstrate the simple chain (no tools) for comparison, (10) generate a batch of affirmations for a practice, (11) save results to JSON using `save_affirmations()`, (12) display formatted results.                                                                                                                                                                                                                                                                                      | ✅        | 2026-02-28 |
| TASK-027  | Create `applications/affirmations/output/.gitkeep` to track the output directory in git while ignoring generated JSON files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | ✅        | 2026-02-28 |

### Implementation Phase 5 — Testing & Validation

- GOAL-005: Create unit tests with mocked LLM and tool responses to validate chain construction, tool setup, agent graph, output serialization, and Pydantic models without requiring a running Ollama instance or network access.

| Task      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                        | Completed | Date       |
| --------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---------- |
| TASK-028  | Create `applications/affirmations/testing/__init__.py` (empty).                                                                                                                                                                                                                                                                                                                                                                                    | ✅        | 2026-02-28 |
| TASK-029  | Create `applications/affirmations/testing/test_models_unit.py`. **Deviation**: Uses `pytest.raises(ValidationError)` (imported from pydantic) instead of `pytest.raises(Exception)` for precise error type assertions.                                                                                                                                                                                                                             | ✅        | 2026-02-28 |
| TASK-030  | Create `applications/affirmations/testing/test_output_unit.py`. Uses `tmp_path: pathlib.Path` fixture (pytest built-in) for isolated file I/O.                                                                                                                                                                                                                                                                                                     | ✅        | 2026-02-28 |
| TASK-031  | Create `applications/affirmations/testing/test_chains_unit.py`. Patches `chain.invoke` directly since the LCEL pipe composite type doesn't expose easy injection points for the mock LLM response.                                                                                                                                                                                                                                                 | ✅        | 2026-02-28 |
| TASK-032  | Create `applications/affirmations/testing/test_tools_unit.py`. Verifies `create_tools()` returns exactly 2 tools (SearxNG, Wikipedia). Uses a `_make_mock_tool` helper to reduce boilerplate. Combined `with patch.dict` + `with patch` into single combined `with` statements (Ruff SIM117 compliance). **Deviation from original plan**: DuckDuckGo and Jina Reader removed — no longer tested for conditional Jina loading or DDG availability. | ✅        | 2026-02-28 |
| TASK-032b | Create `applications/affirmations/testing/test_research_unit.py`. 27 tests covering all 6 functions in `research.py`. `extract_content` tests allow both successful Trafilatura extraction and graceful empty-string fallback (Trafilatura requires real HTML structure to extract content). 93% line coverage on `src/` overall.                                                                                                                  | ✅        | 2026-02-28 |
| TASK-033  | Create `applications/affirmations/testing/test_agent_unit.py`. Patches `langgraph.prebuilt.create_react_agent` to return a mock compiled graph; verifies the function is called with the correct `model` and `tools` arguments.                                                                                                                                                                                                                    | ✅        | 2026-02-28 |
| TASK-034  | Create `applications/affirmations/testing/test_practices_unit.py`. **Deviation**: Tests cover all 6 practices (not only the originally-seeded 3) since all were implemented. Validates all zodiac signs, 7 chakras, and Major Arcana spot-checks.                                                                                                                                                                                                  | ✅        | 2026-02-28 |

### Implementation Phase 6 — Documentation & Polish

- GOAL-006: Finalize documentation, README, and ensure the project is ready for use.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Completed   | Date       |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ---------- |
| TASK-035 | Create `applications/affirmations/README.md`: Project overview, prerequisites (Docker, uv), quickstart guide, project structure diagram, Nx targets table, output format, tools table, research processing layer explanation, adding new spiritual practices guide, services table, environment variables table, performance notes.                                                                                                                                                                                                                                                                                                                                                                           | ✅          | 2026-02-28 |
| TASK-036 | Update root `AGENTS.md` projects section: Added `affirmations` entry and added `### Affirmations (Python + Ollama)` subsection to Quick Workflows with the 5 key commands.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | ✅          | 2026-02-28 |
| TASK-037 | No new skill warranted for Phase 1 — `langchain-python.md` framework doc and `ephemeris-pipeline` skill cover the relevant patterns. The `affirmations` AGENTS.md documents all project-specific conventions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | ✅          | 2026-02-28 |
| TASK-038 | All Nx targets verified passing: `nx run affirmations:lint` (ruff — all checks passed), `nx run affirmations:format` (ruff format — 0 issues on clean runs), `nx run affirmations:typecheck` (pyright strict — 0 errors, 0 warnings in 9 source files), `nx run affirmations:vulture` (vulture — 0 findings), `nx run affirmations:test` (52 passed, 95% coverage), `nx run affirmations:spell-check` ✅, `nx run affirmations:markdown-lint` ✅, `nx run affirmations:yaml-lint` ✅. Several fixes applied: Ruff `I001` (import sort), `SIM117` (combine `with` statements), pyright overrides for `WikipediaAPIWrapper`, `CompiledStateGraph` type param, `B018` false positive on `.vulture_whitelist.py`. | ✅          | 2026-02-28 |
| TASK-039 | End-to-end notebook validation against a live Ollama + SearxNG instance. **Deferred** — requires `nx run affirmations:setup` (Docker service startup + 3.3GB model download). Infrastructure is in place; manual validation step for when the Docker environment is active.                                                                                                                                                                                                                                                                                                                                                                                                                                   | ⏳ Deferred |            |

### Implementation Phase 7 — Tooling Enhancements & Service UX

- GOAL-007: Replace deprecated tools (DuckDuckGo, Jina, mypy) with preferred alternatives (SearxNG-only search, trafilatura-only content extraction, pyright type-checking), add dead code analysis, and provide browser-accessible UIs for Ollama (Open WebUI) and SearxNG.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Completed | Date       |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---------- |
| TASK-040 | Replace **mypy** with **pyright** for type checking. Update `pyproject.toml`: remove `[tool.mypy]` and all `[[tool.mypy.overrides]]` sections; add `[tool.pyright]` with `pythonVersion = "3.11"`, `typeCheckingMode = "strict"`, `venvPath = "."`, `venv = ".venv"`. Replace `mypy` with `pyright` in dev dependencies. Update `project.json` `typecheck` target: `uv run pyright src/`. Run `uv sync` to install pyright. Validate with `nx run affirmations:typecheck`. Update `AGENTS.md` and `README.md` to reference pyright.                                                                                                                                   | ✅        | 2026-02-28 |
| TASK-041 | Add **vulture** for dead code analysis. Add `vulture` to dev dependencies in `pyproject.toml`. Add Nx target `vulture` in `project.json`: `uv run vulture src/ .vulture_whitelist.py --min-confidence 80`. Create `applications/affirmations/.vulture_whitelist.py` to suppress false positives for Pydantic model fields and LangChain duck-typing patterns. Run `uv sync` to install. Add `vulture` to `AGENTS.md` and `README.md` Nx targets tables. Add `vulture` to `cspell.config.yaml`.                                                                                                                                                                        | ✅        | 2026-02-28 |
| TASK-042 | Remove **DuckDuckGo** as a search tool. Remove `duckduckgo-search` from `pyproject.toml` dependencies. Update `src/tools.py`: remove `web_search` tool (DuckDuckGoSearchRun wrapper), update `create_tools()` to return only `[searxng_search, wikipedia_lookup]`. Update `test_tools_unit.py` to expect exactly 2 tools. Remove DuckDuckGo from `AGENTS.md` and `README.md` tool tables. Run `uv sync` to remove the package.                                                                                                                                                                                                                                        | ✅        | 2026-02-28 |
| TASK-043 | Remove **Jina Reader** as a tool. Remove `langchain-community[jina]` optional dependency from `pyproject.toml`. Remove `jina_reader` tool definition and `JINA_API_KEY` conditional check from `src/tools.py`. Update `test_tools_unit.py` to remove Jina test cases. Content extraction from URLs is handled exclusively by **Trafilatura** in `src/research.py`. Update `AGENTS.md` and `README.md` documentation.                                                                                                                                                                                                                                                  | ✅        | 2026-02-28 |
| TASK-044 | Add **Open WebUI** as a Docker Compose service for interacting with the Ollama model via a web browser. Add service to `docker-compose.yml`: image `ghcr.io/open-webui/open-webui:main`, container name `open-webui`, port mapping `3001:8080` (host port 3001 to avoid conflict with Lexico dev server on port 3000), environment `OLLAMA_BASE_URL=http://ollama:11434` (connects to `ollama` container within the Docker network), volume `open-webui_data:/app/backend/data`, `depends_on: [ollama]`, restart `unless-stopped`. Add `open-webui_data` named volume. Add port `3001` to `.devcontainer/devcontainer.json` `forwardPorts` with label `"Open WebUI"`. | ✅        | 2026-02-28 |
| TASK-045 | Add Nx targets for opening web UIs in VSCode's integrated browser. In `project.json`: `open-webui` target → `command: "$BROWSER http://localhost:3001"`, cwd `applications/affirmations`. `open-searxng` target → `command: "$BROWSER http://localhost:8889"`, cwd `applications/affirmations`. The `$BROWSER` environment variable opens URLs in VSCode's integrated Simple Browser when running in a devcontainer. Update `setup-ollama.sh` to also start `open-webui`. Update `AGENTS.md` and `README.md` with the new targets.                                                                                                                                    | ✅        | 2026-02-28 |
| TASK-046 | Add new terms to `cspell.config.yaml`: `vulture`, `pyright`, `openwebui`. Remove `mypy` if no longer referenced elsewhere in the monorepo. Also update `AGENTS.md` Quick Workflows section to include `nx run affirmations:vulture`, `nx run affirmations:open-webui`, and `nx run affirmations:open-searxng`.                                                                                                                                                                                                                                                                                                                                                        | ✅        | 2026-02-28 |

## 3. Alternatives

- **ALT-001**: **Use `pip` + `requirements.txt` instead of `uv`** — Simpler and already available in the devcontainer, but lacks lockfile reproducibility, slow installs, and doesn't manage virtual environments as elegantly. Rejected in favor of `uv` for its speed, lockfile support (`uv.lock`), and modern ecosystem alignment.

- **ALT-002**: **Use `poetry` instead of `uv`** — Mature lockfile-based manager with wide adoption, but significantly slower than `uv`, heavier setup, and the monorepo has no existing Poetry configuration. Rejected for the same reasons as ALT-001 plus added complexity.

- **ALT-003**: **Run Ollama natively (not in Docker)** — Would simplify networking but breaks the Docker-in-Docker pattern established in the devcontainer and makes the setup non-portable. Rejected because Docker Compose is the existing service pattern and ensures consistent environments.

- **ALT-004**: **Use OpenAI API or other cloud LLM instead of local Ollama** — Would provide higher quality outputs and avoid model download, but introduces API key management, costs, rate limits, and internet dependency. Rejected because the requirement specifies a fully local setup with Gemma on Ollama.

- **ALT-005**: **Use plain Python scripts instead of Jupyter notebooks** — Simpler project structure, easier to lint/test, but loses the interactive exploration workflow that is core to prompt engineering and iterating on affirmation structures. Rejected because Jupyter is the natural interface for LLM prompt development.

- **ALT-006**: **Use `gemma3:1b` (815MB) for faster setup** — Significantly smaller download and faster inference, but research indicates smaller models struggle with structured output and JSON schema compliance. Rejected in favor of `gemma3:4b` which offers reliable structured output with 128K context at a reasonable 3.3GB size.

- **ALT-007**: **Add a minimal `package.json` for Nx workspace detection** — pnpm workspaces auto-detect `applications/*` and typically expect `package.json`. However, Nx can discover projects via `project.json` alone when listed in `nx.json` workspace configuration. Using `project.json`-only is cleaner for a Python project. If Nx discovery issues arise, a minimal `package.json` can be added as a fallback.

- **ALT-008**: **Use Tavily Search as a search tool** — Tavily provides a purpose-built search API for LLM agents with better result quality and pre-summarized answers. However, it requires an API key with a limited free tier (1,000 searches/month, then paid). **Deferred to a future phase** when the application is mature enough to justify the investment. For now, SearxNG + Wikipedia provide comprehensive coverage without commercial API dependencies.

- **ALT-009**: **Use SerpAPI or Google Search instead of SearxNG** — Higher quality search results from Google, but both require API keys and have usage limits. Rejected — SearxNG provides access to Google results via its metasearch aggregation without requiring a Google API key.

- **ALT-012**: **Use SearxNG as the only search tool (replacing DuckDuckGo + Wikipedia)** — **Partially adopted**: DuckDuckGo has been removed in favor of SearxNG (which subsumes it via metasearch aggregation). Wikipedia is retained as a dedicated tool for authoritative, encyclopedic reference material — its clean structured output and direct article lookup behavior remains valuable alongside SearxNG's broader web search.

- **ALT-013**: **Use LLM-based summarization instead of programmatic processing for the research layer** — An LLM could produce higher quality summaries of search results. However, with a 4B model on CPU, each summarization call adds 30-60s latency. Programmatic processing (Trafilatura extraction + text processing) is instant and deterministic. Rejected for performance reasons — the programmatic approach handles 95% of cases well enough.

- **ALT-014**: **Skip the research processing layer, pass raw results to the LLM** — Simpler implementation, but raw search results contain HTML fragments, boilerplate, duplicate information, and excessive length that overwhelm a 4B model's limited context window. Rejected because the Trafilatura-powered processing layer is essential for reliable output quality with a small model.

- **ALT-015**: **Use Crawl4AI instead of Trafilatura for URL content extraction** — Crawl4AI (61k GitHub stars) is a powerful open-source LLM-friendly web crawler with async browser support and deep crawling capabilities. However, it requires Playwright/Chromium installation (heavy dependency), is designed for large-scale crawling rather than single-page extraction, and adds significant complexity. Rejected — Trafilatura (local, no browser/API needed) handles all content extraction needs without external dependencies.

- **ALT-016**: **Use Firecrawl instead of Trafilatura** — Firecrawl is a commercial web scraping API that produces LLM-ready markdown. Rejected — requires an API key and has commercial usage limits. Trafilatura is free, local, open-source, and produces equally good LLM-optimized output without any API dependency.

- **ALT-017**: **Use BeautifulSoup instead of Trafilatura for HTML content extraction** — BeautifulSoup is simpler and already widely known. However, it only strips HTML tags — it doesn't identify main content vs. boilerplate (navs, footers, ads, sidebars). Trafilatura uses jusText + readability algorithms to extract only the main article content, which is strictly superior for research processing. Trafilatura outperforms all other open-source extractors in multiple independent benchmarks (used by HuggingFace, IBM, Microsoft Research, Stanford).

- **ALT-018**: **Use mypy instead of pyright for type checking** — mypy is the canonical Python type checker with the largest ecosystem of type stubs. However, pyright offers stricter enforcement, significantly faster execution, better IDE integration (it powers Pylance in VSCode), and handles complex typing patterns without needing per-package `[[overrides]]` stubs. Rejected — pyright's VSCode-native integration and speed make it the superior choice for this developer-experience-focused project.

- **ALT-019**: **Use Ruff's dead code rules instead of vulture** — Ruff detects `F401` (unused imports), `F841` (unused variables), and `F811` (redefinition of unused variables), but does not detect unused functions or classes. Vulture performs full dead code analysis including unreachable functions, classes, and attributes. Both are used: Ruff for import/variable cleanup (already configured), vulture for module-level dead code. **Adopted**: use both complementarily.

- **ALT-020**: **Use Ollama's built-in web interface instead of Open WebUI** — Ollama exposes a REST API but has no built-in UI. Open WebUI provides a full ChatGPT-like interface with conversation history, system prompt editing, model switching, and parameter tuning — all out of the box as a Docker service. Rejected (the native approach) in favor of Open WebUI for its richer feature set.

- **ALT-010**: **Skip web search tools, rely only on LLM's training data** — Simpler architecture (no agent loop needed), but the LLM's knowledge of niche spiritual practices may be incomplete or outdated. Rejected because research-augmented generation produces significantly richer and more accurate affirmations, especially for less common practices like lenormand and kabbalah.

- **ALT-011**: **Use a simple sequential chain (research → generate) instead of a ReAct agent** — Deterministic flow where every generation first searches, then generates. Simpler but inflexible — the agent can't decide whether research is needed or iterate. Rejected in favor of LangGraph ReAct agent which gives the LLM autonomy to research as needed.

## 4. Dependencies

### External Dependencies (Python packages)

- **DEP-001**: `langchain~=1.2` — Core LangChain framework for chain composition and LCEL
- **DEP-002**: `langchain-ollama~=1.0` — Official Ollama integration providing `ChatOllama`
- **DEP-003**: `langchain-community~=1.0` — Community integrations providing `WikipediaQueryRun`, `WikipediaAPIWrapper`, and `SearxSearchWrapper`. DuckDuckGoSearchRun and JinaSearch removed
- **DEP-004**: `langgraph~=1.0` — Graph-based agent framework for ReAct agent with tool calling
- **DEP-005**: `pydantic~=2.0` — Data validation and structured output schemas
- **DEP-006**: `jupyter~=1.1` — Notebook server and interface
- **DEP-007**: `ipykernel~=6.29` — Python kernel for Jupyter notebooks
- **DEP-009**: `wikipedia~=1.4` — Python Wikipedia API client for `WikipediaQueryRun` (no API key required)
- **DEP-022**: `trafilatura~=2.0` — Best-in-class HTML content extraction for the research processing layer. Extracts main article content from raw HTML, removing boilerplate (navs, footers, ads). Outputs clean text/markdown. Used by HuggingFace, IBM, Microsoft Research, Stanford. Apache 2.0 license. Replaces BeautifulSoup for this use case

### Dev Dependencies (Python packages)

- **DEP-010**: `ruff` — Linting and formatting (replaces ESLint/Biome/Prettier for Python)
- **DEP-011**: `pyright` — Static type checking (replaces TypeScript `tsc`; supersedes mypy with better IDE integration and speed)
- **DEP-012**: `pytest` — Test framework (replaces Vitest)
- **DEP-013**: `pytest-cov` — Coverage reporting for pytest
- **DEP-025**: `vulture` — Dead code detection for Python source files. Identifies unused functions, classes, and variables not caught by Ruff's narrower checks (`F401`/`F841`)

### Infrastructure Dependencies

- **DEP-014**: `ollama/ollama:latest` Docker image (≥0.6 for Gemma 3 support)
- **DEP-015**: `gemma3:4b` model (3.3GB download, pulled at setup time)
- **DEP-016**: `uv` — Python package manager (must be installed in devcontainer or globally)
- **DEP-017**: Docker-in-Docker — Already enabled in `.devcontainer/devcontainer.json`
- **DEP-024**: `searxng/searxng:latest` Docker image — Self-hosted metasearch engine, aggregates 135+ search engines (Wikipedia, DuckDuckGo, Google Scholar, ArXiv, GitHub, Wikidata, etc.) via a single JSON API. ~50MB Docker image, no API keys, no rate limits from upstream providers
- **DEP-026**: `ghcr.io/open-webui/open-webui:main` Docker image — Full-featured ChatGPT-like web interface for Ollama. ~1GB Docker image. Provides conversation history, system prompt editing, model switching, and parameter tuning via browser at `http://localhost:3000`

### Monorepo Dependencies

- **DEP-018**: `nx` — Task runner for project targets
- **DEP-019**: `docker-compose.yml` — Service orchestration for Ollama
- **DEP-020**: `cspell.config.yaml` — Spell checking configuration (must add domain terms)
- **DEP-021**: `conventional.config.cjs` — Commit scope validation (must add `affirmations`)

## 5. Files

### New Files

- **FILE-001**: `applications/affirmations/project.json` — Nx project configuration with Python-specific targets
- **FILE-002**: `applications/affirmations/AGENTS.md` — Copilot agent instructions for the affirmations project
- **FILE-003**: `applications/affirmations/pyproject.toml` — Python project metadata, dependencies, and tool configuration (Ruff, pyright, vulture, pytest)
- **FILE-004**: `applications/affirmations/.gitignore` — Python-specific gitignore rules
- **FILE-005**: `applications/affirmations/README.md` — Project documentation and quickstart guide
- **FILE-006**: `applications/affirmations/src/__init__.py` — Package marker
- **FILE-007**: `applications/affirmations/src/models.py` — Pydantic models for `Affirmation` and `AffirmationSet`
- **FILE-008**: `applications/affirmations/src/llm.py` — `ChatOllama` factory function
- **FILE-009**: `applications/affirmations/src/tools.py` — Search tool definitions (SearxNG, Wikipedia) with `create_tools()` factory. DuckDuckGo and Jina Reader removed
- **FILE-009b**: `applications/affirmations/src/research.py` — Research processing layer: Trafilatura-powered content extraction, relevance truncation, deduplication, context budgeting, and consistent formatting for LLM consumption
- **FILE-010**: `applications/affirmations/src/chains.py` — Simple LCEL chain for direct affirmation generation (no tools)
- **FILE-011**: `applications/affirmations/src/agent.py` — LangGraph ReAct agent with research tools for research-augmented affirmation generation
- **FILE-012**: `applications/affirmations/src/output.py` — JSON file read/write utilities
- **FILE-013**: `applications/affirmations/src/practices.py` — Spiritual practice configuration data
- **FILE-014**: `applications/affirmations/notebooks/example-affirmation-generation.ipynb` — Example Jupyter notebook demonstrating both the simple chain and research agent workflows
- **FILE-015**: `applications/affirmations/output/.gitkeep` — Placeholder to track output directory
- **FILE-016**: `applications/affirmations/scripts/setup-ollama.sh` — Convenience script for Ollama setup and model pull
- **FILE-017**: `applications/affirmations/testing/__init__.py` — Test package marker
- **FILE-018**: `applications/affirmations/testing/test_models_unit.py` — Pydantic model unit tests
- **FILE-019**: `applications/affirmations/testing/test_output_unit.py` — Output utility unit tests
- **FILE-020**: `applications/affirmations/testing/test_chains_unit.py` — Chain construction unit tests
- **FILE-021**: `applications/affirmations/testing/test_tools_unit.py` — Search tool unit tests (SearxNG, Wikipedia). DuckDuckGo and Jina Reader tests removed
- **FILE-021b**: `applications/affirmations/testing/test_research_unit.py` — Research processing layer unit tests (Trafilatura extraction, truncation, deduplication, context budgeting)
- **FILE-022**: `applications/affirmations/testing/test_agent_unit.py` — LangGraph ReAct agent unit tests
- **FILE-023**: `applications/affirmations/testing/test_practices_unit.py` — Practice configuration unit tests

- **FILE-029**: `applications/affirmations/.vulture_whitelist.py` — Vulture whitelist to suppress false positives for Pydantic model fields and LangChain duck-typing patterns (new in Phase 7)

### Modified Files

- **FILE-024**: `docker-compose.yml` — Add `ollama`, `searxng`, and `open-webui` services, `ollama_data` and `open-webui_data` volumes
- **FILE-024b**: `applications/affirmations/config/searxng/settings.yml` — SearxNG engine configuration (enabled engines, JSON output, rate limits)
- **FILE-025**: `conventional.config.cjs` — Add `"affirmations"` to scopes array
- **FILE-026**: `cspell.config.yaml` — Add domain-specific dictionary words
- **FILE-027**: `knip.config.ts` — Add `applications/affirmations` to `ignoreWorkspaces`
- **FILE-028**: `AGENTS.md` (root) — Add affirmations project entry to projects section and quick workflows

## 6. Testing

### Unit Tests (mocked, no Ollama required)

- **TEST-001**: `test_models_unit.py` — `Affirmation` model validates correct fields, rejects missing `text`, rejects invalid types. `AffirmationSet` serializes to JSON with `model_dump_json()` and deserializes back with `model_validate_json()`.
- **TEST-002**: `test_output_unit.py` — `save_affirmations` writes valid JSON to the correct path in `output/`. `load_affirmations` reads and parses JSON back to `AffirmationSet`. Handles missing file gracefully with clear error.
- **TEST-003**: `test_chains_unit.py` — `create_affirmation_chain` returns a `RunnableSequence`. Invoking with mocked `ChatOllama` passes correct prompt variables. Structured output parser produces valid `Affirmation` from mocked LLM response.
- **TEST-004**: `test_tools_unit.py` — `create_tools()` returns exactly 2 tools (SearxNG, Wikipedia). Tool names include `"searxng_search"` and `"wikipedia_lookup"`. Each tool has a non-empty description. Mock all underlying clients to return canned results. Verify tool outputs pass through research processing layer. Tools handle empty/error responses gracefully. DuckDuckGo and Jina Reader removed.
- **TEST-004b**: `test_research_unit.py` — `extract_content()` uses Trafilatura to extract main content from raw HTML, removing boilerplate. Falls back gracefully when Trafilatura returns empty. `truncate_with_relevance()` keeps query-relevant sections within `max_chars`. `format_research_result()` produces `[Source: X] Title: Content` format. `deduplicate_results()` removes overlapping content across multiple sources. `budget_context()` enforces total token limit and distributes proportionally. `process_search_results()` end-to-end: raw HTML in → clean, Trafilatura-extracted, truncated, formatted text out.
- **TEST-005**: `test_agent_unit.py` — `create_research_agent` returns a compiled `StateGraph`. When invoked with mocked LLM + mocked tools, the agent executes at least one tool call before generating the final response. The final message contains an affirmation. Agent handles tool errors without crashing.
- **TEST-006**: `test_practices_unit.py` — All entries in `PRACTICES` dict have non-empty `topics` and `structures` lists. All practice names are lowercase kebab-case. `PracticeConfig` rejects empty topic lists.

### Integration Tests (future, requires running Ollama + network)

- **TEST-007**: End-to-end notebook execution with `nbconvert` — verify the example notebook runs without errors against a live Ollama instance with web search and Wikipedia access (deferred to a future phase; marked as a manual validation step for now).

## 7. Risks & Assumptions

### Risks

- **RISK-001**: **Ollama Docker-in-Docker performance** — Running Ollama inside a devcontainer's DinD may have significant performance overhead. CPU-only inference with Gemma 3 4B may be slow (30–60s per generation). Mitigation: Use `OLLAMA_KEEP_ALIVE=10m` to avoid repeated model loads; consider `gemma3:1b` as a fallback for faster iteration.
- **RISK-002**: **Structured output reliability** — Smaller LLMs may not consistently produce valid JSON matching the Pydantic schema. Mitigation: Include the JSON schema in the system prompt text alongside `with_structured_output()` for belt-and-suspenders reinforcement; add retry logic for parsing failures.
- **RISK-007**: **Gemma 3 tool-calling support** — Gemma 3 does not natively appear in Ollama's tool-calling capable models list. The ReAct agent relies on the model understanding tool-call prompts. Mitigation: Use LangGraph's `create_react_agent` which handles tool-call prompting for models without native function calling; test with Gemma 3 4B specifically; fall back to a prompt-based ReAct loop if structured tool calls fail.
- **RISK-008**: **Web search availability in Docker-in-Docker** — SearxNG and Wikipedia require outbound internet access from the devcontainer. Mitigation: Devcontainers typically have internet access; if firewalled, the simple chain (no tools) remains functional as a fallback.
- **RISK-009**: **Open WebUI container resource usage** — Open WebUI adds another Docker container (~1GB image) to the DinD environment. Mitigation: Only start it when needed for interactive prompting via `nx run affirmations:setup`. Can be excluded from the default compose profile if resources are constrained.
- **RISK-010**: **SearxNG Docker resource usage** — SearxNG adds another Docker container to the DinD environment. Mitigation: SearxNG image is lightweight (~50MB), minimal CPU/memory footprint. If resources are constrained, SearxNG can be stopped and Wikipedia remains functional as a fallback for authoritative reference lookups.
- **RISK-011**: **Research processing layer over-truncation** — Aggressive truncation may remove important context. Mitigation: Make `max_chars` and `max_total_tokens` configurable parameters with sensible defaults; log truncation warnings; the LLM can always request additional searches if initial results are insufficient.
- **RISK-012**: **Vulture false positives** — Vulture may flag Pydantic model fields and LangChain duck-typing patterns as unused code (false positives). Mitigation: Create `.vulture_whitelist.py` for known false positives; set `--min-confidence 80` to reduce noise; review output before acting on it.
- **RISK-003**: **uv availability in CI** — GitHub Actions runners don't have `uv` pre-installed. If CI integration is added in a future phase, `uv` must be installed as a setup step. Mitigation: Deferred to future phase; `astral-sh/setup-uv` GitHub Action exists for this purpose.
- **RISK-004**: **Model size and disk space** — Gemma 3 4B is 3.3GB. Docker volumes persist the model, but initial download requires good internet. Mitigation: Document the model pull as a one-time manual step; provide `gemma3:1b` as a smaller alternative.
- **RISK-005**: **First Python project friction** — Establishing Python patterns (Ruff, pyright, vulture, pytest) in a TypeScript-dominant monorepo may surface unexpected Nx/CI integration issues. Mitigation: Keep Phase 1 scope minimal; validate all Nx targets before expanding.
- **RISK-006**: **Nx workspace detection for Python projects** — Without `package.json`, pnpm won't recognize the project as a workspace. Nx may still discover it via `project.json`, but cache invalidation and dependency tracking may behave differently. Mitigation: Test Nx task execution thoroughly in TASK-034; add minimal `package.json` as fallback if needed.

### Assumptions

- **ASSUMPTION-001**: The devcontainer's Docker-in-Docker feature provides sufficient Docker CLI access to run `docker compose up` and manage Ollama containers.
- **ASSUMPTION-002**: Python 3.14 (installed in devcontainer) is compatible with all specified Python packages. LangGraph requires ≥3.11, which is satisfied.
- **ASSUMPTION-003**: `uv` can be installed via `pip install uv` or `curl` within the devcontainer if not already available.
- **ASSUMPTION-004**: The Ollama container can be reached at `http://localhost:11434` from within the devcontainer (port forwarding through DinD).
- **ASSUMPTION-005**: JSON output files are the primary artifact — no database or API server is needed for Phase 1.
- **ASSUMPTION-006**: Spiritual practice data (tarot card names, zodiac signs, chakra names, etc.) will be hardcoded in `practices.py` as initial seed data. The LLM agent will supplement this with live research via web search and Wikipedia at generation time.
- **ASSUMPTION-007**: The devcontainer has outbound internet access for SearxNG (which proxies to upstream engines) and Wikipedia API calls.
- **ASSUMPTION-008**: Open WebUI can reach the `ollama` container within the Docker network via `http://ollama:11434`. Port 3000 is forwarded from the devcontainer to the host for browser access via `$BROWSER`.
- **ASSUMPTION-009**: SearxNG's self-hosted instance can reach upstream search engines (Wikipedia, Google Scholar, ArXiv) from within the Docker network. This requires the same outbound internet access as ASSUMPTION-007.
- **ASSUMPTION-010**: Trafilatura's content extraction algorithms handle the majority of web pages returned by search engines. For edge cases (JavaScript-heavy SPAs), raw text fallback is acceptable.

## 8. Related Specifications / Further Reading

- [documentation/frameworks/langchain-python.md](../../documentation/frameworks/langchain-python.md) — Monorepo's LangChain Python guide (LCEL, ChatOllama, prompt templates)
- [documentation/planning/infrastructure-devcontainers-1.md](../../documentation/planning/infrastructure-devcontainers-1.md) — Completed plan establishing the devcontainer with Python, Jupyter, and DinD
- [LangChain Python Docs](https://docs.langchain.com/oss/python/langchain/overview) — Official LangChain documentation
- [ChatOllama Integration](https://docs.langchain.com/oss/python/integrations/chat/ollama) — LangChain Ollama integration guide
- [LangGraph Docs](https://docs.langchain.com/oss/python/langgraph/overview) — LangGraph framework documentation
- [Ollama Docker Hub](https://hub.docker.com/r/ollama/ollama) — Official Ollama Docker image
- [Gemma 3 on Ollama](https://ollama.com/library/gemma3) — Gemma 3 model card and usage
- [Ollama Environment Variables](https://github.com/ollama/ollama/blob/main/docs/faq.mdx) — Ollama configuration reference
- [uv Documentation](https://docs.astral.sh/uv/) — Python package manager documentation
- [Ruff Documentation](https://docs.astral.sh/ruff/) — Python linter and formatter
- [LangChain Wikipedia Tool](https://python.langchain.com/docs/integrations/tools/wikipedia/) — Wikipedia query integration
- [LangGraph Prebuilt Agents](https://langchain-ai.github.io/langgraph/reference/prebuilt/) — `create_react_agent` documentation
- [Wikipedia PyPI](https://pypi.org/project/wikipedia/) — Python Wikipedia API client
- [SearxNG Documentation](https://docs.searxng.org/) — Self-hosted metasearch engine documentation
- [SearxNG Docker Hub](https://hub.docker.com/r/searxng/searxng) — Official SearxNG Docker image
- [SearxNG Search API](https://docs.searxng.org/dev/search_api.html) — SearxNG JSON API reference
- [LangChain SearxNG Integration](https://docs.langchain.com/oss/python/integrations/tools/searx_search) — LangChain SearxSearchWrapper documentation
- [LangChain Tavily Integration](https://docs.langchain.com/oss/python/integrations/tools/tavily_search) — Tavily search tool documentation (deferred to future phase)
- [Tavily API](https://docs.tavily.com/) — Tavily search API reference (deferred — 1,000 free searches/month, then paid)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) — HTML parsing library (replaced by Trafilatura for content extraction)
- [Trafilatura Documentation](https://trafilatura.readthedocs.io/) — Best-in-class web content extraction library
- [Trafilatura GitHub](https://github.com/adbar/trafilatura) — Open-source HTML→text/markdown extractor (5.4k stars, Apache 2.0)
- [pyright Documentation](https://github.com/microsoft/pyright) — Microsoft's fast Python static type checker (powers Pylance in VSCode)
- [pyright Configuration Reference](https://github.com/microsoft/pyright/blob/main/docs/configuration.md) — pyproject.toml `[tool.pyright]` settings reference
- [vulture Documentation](https://github.com/jendrikseipp/vulture) — Dead code detection for Python (unused functions, classes, variables)
- [Open WebUI GitHub](https://github.com/open-webui/open-webui) — Full-featured ChatGPT-like web interface for Ollama
- [Open WebUI Docker Hub](https://hub.docker.com/r/ghcr.io/open-webui/open-webui) — Official Open WebUI Docker image (connects to Ollama via `OLLAMA_BASE_URL`)
